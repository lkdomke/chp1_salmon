---
title: "chp1_sal_and_vegetation"
author: "Lia Domke"
date: "9/3/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

For chapter 1 I'm focusing on salmon in nearshore habitats. Based on the brief lit review in Lefcheck 2019 and their subsequent analysis that did *not* include any juvenile fish density comparison surveys from Alaska, I'm curious to look at salmon density and composition (which sp present) between different nearshore habitats (unvegtation: bedrock/sand-gravel) (vegetated: eelgrass/understory kelp)
Steps to do this:
1. subset noaa data by only sites that sampled different adjacent habitats around the same time. 

2. Look into the timing of the salmon catch, include on earlier summer catches. 

3. Include biomass too, calculate biomass for each of the salmon species caught based on the fork length. 

# Libraries
```{r libraries}
library(dplyr)
require(leaflet)
require(rgdal)
library(corrgram)
library(lubridate)
library(tidyr)
library(ggplot2)
library(visreg)
```

# Data
```{r data}
# this dataset has the distance from anad stream which is good, but for some reason a couple sites where bedrock was seined there is no distance measurement. 
anad_dist <- read.csv("../APECS Master repository/APECS Master repo/ALL_DATA/Lia_fish/noaa_seine_clean_dist.csv", 
                 stringsAsFactors = FALSE, header = TRUE) 

# use this df for all seine data (as of 9/16/20)
noaa <- read.csv("../APECS Master repository/APECS Master repo/ALL_DATA/Lia_fish/noaa_seak_fish_atlas_CLEAN_LD.csv")

fishLW <- read.csv("https://knb.ecoinformatics.org/knb/d1/mn/v2/object/urn%3Auuid%3A42b3eee9-f6a3-4169-99ef-ed388f46d172", stringsAsFactors = FALSE, header = TRUE)
```

# Cleaning
Minor cleaning to make sure we're only looking at Southeast data and that there are 0's for sites were we didn't catch any salmon
Subset data to include only salmon data
```{r cleaning}
# do this now to prevent having to do it 100 times below
noaa$SiteID <- as.character(noaa$SiteID)
noaa$EventID <- as.character(noaa$EventID)

# double check to make sure we're only looking at SEAK and BSEINE gear
seak_fish <- noaa %>%
  dplyr::filter(Region == "southeastern Alaska", Gear == "BSEINE") %>%
  dplyr::rename(Latitude = Lat1, Longitude = Long1)

# Extract environmental only data 
env <- seak_fish %>%
  dplyr::select(SiteID, EventID, Date, Year, Temp, Salinity, Habitat, Latitude, Longitude) %>%
  distinct()

# location data
loc <- seak_fish %>%
  #rename(Location = Location.y, SubLocale = SubLocale.y) %>%
  dplyr::select(SiteID, EventID, Date, Year, Gear, Region, Locale, Location, SubLocale) %>%
  distinct()

# Calculate fish abundance, can later sum this if interested in only abundance
str(seak_fish)
seak_fish$abundance <- as.numeric(ifelse(is.na(seak_fish$Length), paste(seak_fish$Unmeasured), 1)) 


# extract full list of site info (SiteID, EventID)
SiteID <- unique(seak_fish$SiteID) # 303 unique sites (updated #396)
EventID <- unique(seak_fish$EventID) # 615 total seines done (updated #798)
Site_info <- unique(subset(seak_fish, select = c("SiteID", "EventID", "Latitude", "Longitude", "Locale", "Habitat"))) # all unique seines
```

# Data prep
Want to run a loop so that, for each species caught, we can estimate fork length for the unmeausred fish but also estimate biomass for both. 
```{r}
# make sure columns have appropriate character type
glimpse(seak_fish)
seak_fish$Unmeasured <- as.numeric(seak_fish$Unmeasured)
seak_fish$EventID <- as.character(seak_fish$EventID)

# only need salmon, otherwise there are a lot of fish we don't have length-weight info for:
seak_sal <- seak_fish %>%
  filter(taxon != "invertebrate") %>%
  filter(SpCode == "SALCHUM" | SpCode == "SALCOHO"| 
      SpCode == "SALSOCK"| SpCode == "SALCHIN"
      | SpCode == "SALPINK")

# How many salmon total did we catch? 
pre_loop <- seak_sal %>% 
  dplyr::summarise(count = sum(abundance)) # (# updated seines 149329)

pre_site_sp <- seak_sal %>%
  group_by(SiteID, EventID, SpCode) %>%
  dplyr::summarise(pre_count = sum(abundance))
## Separate measured and unmeasured fish

fish.m <- seak_sal %>%
  filter(taxon != "invertebrate") %>% # make sure you're only looking at fish
  filter(Length != "NA")


fish.um <- seak_sal %>%
  filter(taxon != "invertebrate") %>% 
  filter(Unmeasured != "estimate") %>% # assume that infield estimates are accurate
  filter(is.na(Length)) 

## Assign lengths to unmeasured fish
#' When beach seining we only measured the first 30 individuals of a species, and counted the rest. 
#' We can use the measured fishes to create a distribution from which we can assign 
#' lengths to the unmeasured fishes.

#' Assign lengths to unmeasured fish based on sampled distribution.
#'  This assignment should happen at the EventID level. i.e. use the distribution of fishes at a 
#'  EventID to assign unmeasured fishes at that EventID. 

#' We will assume the least and just use the sampled proportions to assign lenghts to unmeasured fish. 
# Exclued fishes that do not have a measured counterpart. These were insantces when we 
# tossed a fish without knowing what it was other than it was a sculpin thing


# figure out which species at sites that there is not a measured conterpart 
#  x <- fish.um %>%
#  group_by(EventID, SpCode) %>%
#  anti_join(fish.m, by = c("EventID", "SpCode")) # clearly there are lots of situations where during a seine they didn't measure any fish and only counted them--making biomass conversions impossible?

# Which fish do we not have length-width conversions?
y <- fish.um %>%
  anti_join(fishLW, by = c("Sp_ScientificName" = "species_scientific")) # Just sockeye salmon, not on fish base for length to weight conversions, very few sockeye salmon were caught so we might have to drop these fish from the analysis. Within the prince of wales area this is 1 fish. 

# remove the sockeye, we'll try and go ahead and do the conversions for the fish we have info for
fish.um.redu <- fish.um %>%
  filter(SpCode != "SALSOCK") #%>%
  #anti_join(x, by = c("EventID", "SpCode")) # this removes the instances that we didn't have measured counterparts for the measured data
```

# Subset where not enough fish measured
Before we run the loop to extrapolate lengths, we should make sure that there are enough measured fish to be able to extrapolate to the unmeasured ones (n = 30). 
```{r}
# make a loop
f <- data.frame() # dataframe to put the ones that meet the requirement

for(s in unique(fish.um.redu$EventID)){
  df.m <- fish.m %>%
    filter(EventID == s)
  df.um <- fish.um.redu %>%
    filter(EventID == s)
  for(i in unique(df.um$SpCode)){
    samp <- df.m %>%
      filter(SpCode == i)
    cond <- length(samp$Length) > 30
    dat1 <- data.frame(EventID = s, SpCode = i, keep = cond)
    f <- rbind(f, dat1)
  }
} # dang thats a lot that don't make the condition

# Create new df for adding back into the data later
excluded.fish <- left_join(fish.um.redu, f) %>%
  filter(keep == FALSE)
  
  
# Remove the sites that don't have enough unmeasured counterparts

fish.um.redu <- left_join(fish.um.redu, f) %>%
  filter(keep == TRUE)


```

# Length loop!
run the loops!
```{r}
d <- data.frame() # empty dataframe to fill with for loop

for(s in unique(fish.um.redu$EventID)){ # cycle through site by unique seines (EventID)
  dat.m <- fish.m %>% # subset measured data
    filter(EventID == s)
  dat.um <- fish.um.redu %>% #subset unmeasured data
    filter(EventID == s)
  for(i in unique(dat.um$SpCode)){ #cycle through species that are in UNMEASURED data
      samp <- dat.m %>% # create sample from which to make distrubution
        filter(SpCode == i)
      unmeas <- dat.um %>% # isolate unmeasured fish
        filter(SpCode == i) %>%
        dplyr::summarise(unmeas = sum(Unmeasured)) # sums more than one entry of unmeasured at the same site/species
      unmeas <- as.numeric(unmeas$unmeas) # save unmeasured value
      dat <- data.frame(size = as.character(samp$Length))
      dat2 <- dat %>% 
        group_by(size) %>% 
        dplyr::summarise(count = n())
      dat2$prob <- (dat2$count/sum(dat2$count))
      dat2$x <- as.numeric(paste(dat2$size))
      fx <- function(n){ # function derived from limits and probabilities of above
        sample(x = (dat2$x), n, replace = TRUE, prob = dat2$prob)
      }
      dat3 <- data.frame(site = s, sp_code = i, fork_length = fx(unmeas))
      d <- rbind(d, dat3)
    }
}  
```

Add in seperate site/extra information that got dropped from the loop. 
```{r}
fish.site <- unique(seak_fish[,c("SiteID","EventID", "Date", "Mon", "Year", "Gear", "Latitude", "Longitude")])
fish.sp <- unique(seak_fish[,c("Sp_CommonName", "Sp_ScientificName", "SpCode", "taxon")])

# Add site details
d.info <- left_join(d, fish.site, by = c("site" = "EventID")) %>%
  dplyr::rename(EventID = site)
# Add species details
d.info <- left_join(d.info, fish.sp, by = c("sp_code" = "SpCode"))
# Test to make sure it combined properly
test <- anti_join(d.info, d, by = "sp_code")
unique(test$sp_code) # SALCHIN and SALSOCK not included, but thats because they weren't caught... and sockeye were removed
```

Add in the fish that were already measured and the fish there are no cooresponding measured counterparts
```{r}
d.info <- d.info %>%
  dplyr::rename(SpCode = sp_code, Length = fork_length)
# add in measured fish that were used in the loop, but don't appear in the d output dataframe
fish.m.all <- bind_rows(d.info, fish.m)

# add in unmeasured fish that did NOT have a measured counter part and were not included in the loop above
# these are the fish excluded and sockeye salmon 

sal.all <- fish.m.all %>%
  bind_rows(excluded.fish) %>%
  bind_rows(y) %>%
  dplyr::select(-c(X, keep))

# check to make sure we have the same number of salmon we started with (125450, updated 149329)
sal.all$abundance <- as.numeric(ifelse(is.na(sal.all$Length), paste(sal.all$Unmeasured), 1))
sal.all %>%
  dplyr::summarise(sum = sum(abundance)) # woo!

# do an full join, meaning that we're retaining all rows. This should join the salmon sites together and then add in non salmon sites
non_sal <- anti_join(Site_info, sal.all)
fish.all <- full_join(sal.all, non_sal)
```

Okay so now we have a dataframe with information about the salmon that were caught (count numbers) and size (only where measured and where it could be extrapolated). It also has siteID and eventID from all the seines that occured. So we can add in zeros for the sites we didn't catch any salmon. 
BUT because the size info is only partial (salmon weren't measured at every site) Length information should be kept seperate

Create two dataframes, one with abundance one with available length info (so a partial dataset of all the caught fish). 

```{r}
# make sure that abundance column is correct
fish.all$abundance <- as.numeric(ifelse(is.na(fish.all$Length), paste(fish.all$Unmeasured), 1))

fish.count <- fish.all %>%
  dplyr::select(SiteID, EventID, Date, Mon, Year, Gear, Latitude, Longitude, SpCode, Sp_CommonName, 
                Sp_ScientificName, abundance, Locale, Habitat) %>%
  group_by(SiteID, EventID, SpCode) %>%
  dplyr::summarise(abundance = sum(abundance))

# quick checks
fish.all %>%
  dplyr::summarise(post_count = sum(abundance, na.rm = TRUE))

fish_lng <- fish.all %>%
  filter(Length != "NA") # sites where there are no lenght data does NOT mean that no salmon were caught there, just that no salmon were measured there and *might have* been caught. 
```

# Get 0's
For the following purposes we're going to use the `fish.count` dataframe

Calculate 0's for sites no salmon were caught
```{r}
# add back in the site info
fish.counts <- left_join(fish.count, env) %>%
  left_join(loc)

# long --> wide
# This includes ALL species (not just salmon). We'll leave this for now because it matches the
# environmental dataframe. But eventually we'll have to remove the non-salmon species. BUT we want
# retain the 0 where a seine was done but no salmon were caught. 
df_wide <- pivot_wider(fish.counts, id_cols = c(SiteID, EventID, Locale, SubLocale, Date, Year, Latitude, Longitude, Habitat), names_from = SpCode, values_from = abundance) %>%
  dplyr::select(-"NA") %>%
  replace(is.na(.), 0)

# wide -- > long, convserves zeros and site/event ID information

df_long <- pivot_longer(df_wide, cols = c(SALCHIN, SALCOHO, SALPINK, SALCHUM, SALSOCK), names_to = "SpCode", values_to = "abundance")


# REMOVE ALL SEINES WHERE THEY CAUGHT LESS THAN 10 SALMON
# sal_wide <- sal_wide[which(rowSums(sal_wide[,7:11]) > 10),] # this is good to do for nmds analysis
# sal_long <- melt(sal_wide, id = c("SiteID", "EventID", "Date", "Mon", "Season", "Year"), variable.name = "SpCode", value.name = "abundance" )
```

Now we have four dataframes to be working with. 
1. df_wide - salmon abundances for all sites in a wide format
2. df_long - same data as the wide df but long
3. env - environmental data in a wide format
4. loc - location information for each site (included with df_wide and df_long)

# Map
Lets map out the sites so that we can see where the diff sites are location and we can 
```{r map}
# make sure the habitat column is a factor with only 4 levels
fish.counts$Habitat <- as.factor(fish.counts$Habitat)
levels(fish.counts$Habitat)[levels(fish.counts$Habitat)=="Sand-Gravel"] <- "Sand-gravel"
levels(fish.counts$Habitat)[levels(fish.counts$Habitat)=="Surfgrass"] <- "Kelp"

# Prince of Wales Water Polygon
h2o.utm <- readOGR(dsn = "../APECS Master repository/APECS Master repo/ALL_DATA/spatial_data", layer = "POW_water_UTM")
h2o.latlong <- spTransform(h2o.utm, CRS("+init=epsg:4326"))

# Set up icons
icon.hab <- awesomeIcons(icon = "map_pin", 
                          markerColor = ifelse(fish.counts$Habitat == "Kelp", "orange", 
                                               ifelse(fish.counts$Habitat == "Eelgrass", "green",
                                                      ifelse(fish.counts$Habitat == "Bedrock", "gray",
                                                             ifelse(fish.counts$Habitat == "Sand-gravel", "red", "blue")))
                                               ), library = "fa", iconColor = "black")

# Create the map
require(leaflet)
m <- leaflet() %>% 
  addTiles() %>% 
  addPolygons(data = h2o.latlong, stroke = NA, fillColor = "blue", 
                                  fillOpacity = 0.8, group = "POW") %>%
  addAwesomeMarkers(data = fish.counts, ~Longitude, ~Latitude, label = ~SiteID, icon = icon.hab)
# require(htmlwidgets)
# saveWidget(m, file="m.html")
m
```

Based on looking at the map and information about how Johnson et al., 2012 defines southern Southeast Outside there are 8 paired sites that are good comparisons of vegetated versus unvegated habitat. Vegetation either means eelgrass or understory kelp whereas unvegetated means bedrock or sand/gravel. Only *two* sites in Klawock Inlet/Fish Egg area have a sand/gravel - eelgrass comparison. The rest of the 6 sites are bedrock (unveg) versus kelp AND eelgrass.

Additionally, there are a few other "maybe" sites. One site (EventID 573 and 574) is a bedrock-eelgrass comparison on close but seperate islands in the South Wadleigh area. Another maybe site involves a bedrock site (EventID 575) that is located on Wadleigh Island *across* Klawock Inlet from kelp sites (402/403) and eelgrass sites (364/365). 

First lets subset the "all sites data" into just the sites that we can pair so we can check timing of seines. 
# Pair sites
```{r}
# Did the comparison sites happen within the same month/year? 
# One of the "maybe sites from above (EventIDs 575 w/ 402, 403, 364, 365) shouldn't be included because
# the seines in the unveg site happened in June '98 whereas the others happened in July '99. Could do a 
# comparison between eelgrass and kelp but thats it. The other maybe can be included. Drop the bad site

# Also, two of the klawock sites happened in SEPTEMBER. There's no chance of seeing salmon in the nearshore that late in summer, drop 248, 241, 240
# Lets create a list we can subset the data by

EventID_comp <- c(244, 245, 240, 241, 571, 572, 3168, 3170, 3169, 3173, 3172, 3174, 3171, 3181, 3179, 3180, 3177, 3178, 3175, 3176,  3188, 3186, 3187, 3191, 3189, 3190, 573, 574)

# subset the data
fish.count.sub <- subset(fish.counts, EventID %in% EventID_comp)

# but wait what if there were more than one seine at this site?
SiteID_comp <- fish.count.sub$SiteID
fish.sub <- subset(fish.counts, SiteID %in% SiteID_comp) # yea in particular in klawock inlet
# include only the klawock inlet seines that happened in May (EventIDs 244/245 and 236/237)
# create dataframe to antijoin
drop <- fish.sub %>%
  filter(SiteID == "133" & EventID != "244" | 
           SiteID == "134" & EventID != "245"|
           SiteID == "131" & EventID != "236"|
           SiteID == "132" & EventID != "237")

fish.sub <- fish.sub %>%
  anti_join(drop)

# finaaallly lets create a column that pairs the sites
pair.fish <- fish.sub %>%
  group_by(Locale) %>%
  mutate(site_pair = ifelse(Locale %in% "Moira Sound", "Moira Sound", 
                            ifelse(Locale %in% "Nichols Bay", "Nicols Bay", 
                                   ifelse(Locale %in% "Kah Shakes Cove", "Kah Shakes Cove",
                                          ifelse(Locale %in% "Reef Harbor", "Reef Harbor",
                                                 ifelse(Locale %in% "Sylburn Harbor", "Sylburn Harbor", 
                                                        ifelse(Locale %in% "Dall Bay", "Dall Bay",
                                                               ifelse(SubLocale %in% "Ballena Island", "Ballena Island",
                                                                      ifelse(SubLocale %in% c("Clam Island","Wadleigh Island"), "South Wadleigh", "Klawock Inlet")))))))))
```

Okay now that we only have the sites we're for sure interested and we made sure that they were done in the same timeframe (generally within a few days of each other) and that we've paired them by site_pair -- lets do some exploratory analysis of salmon cpue.
# Exploratory graphs
## theme settings
```{r theme settings}
# Creates custom base plot theme that can adjust every graph that you use plot_theme for!

plot_theme <- function() {
  theme_bw(base_size = 14, base_family = "Avenir") %+replace%
    theme(panel.background  = element_blank(),
            plot.background = element_rect(fill="gray96", colour=NA), 
            legend.background = element_rect(fill="transparent", colour=NA),
            legend.key = element_rect(fill="transparent", colour=NA),
            panel.grid.major = element_blank(), 
            panel.grid.minor = element_blank(),
            strip.background = element_rect(colour = "NA", fill = "gray96"))
}

# to use ggsave and export plots include argument 'device=cario_pdf' e.g.: 
# ggsave("name.pdf", device=cairo_pdf, width = 6, height = 6)
```

```{r data compilation}
# Need all the salmon data (all sites we've been working with above) and the associated environmental information
# df for salmon : pair.fish
# environmental data are: temp, salinity, julian day, and anadromous stream dist. 
# anadromous distance comes from df anad_dist
anad_dist$SiteID <- as.character(anad_dist$SiteID)
anad_dist$EventID  <- as.character(anad_dist$EventID)

sal_env <- subset(anad_dist, SiteID %in% SiteID_comp) %>%
  replace(is.na(.), 0) %>%
  dplyr::select(SiteID, EventID, Date, NEAR_DIST, Locale, SubLocale.y) %>%
  distinct() %>%
  right_join(pair.fish) %>%
  mutate(date = mdy(Date)) %>%
  mutate(julian = yday(date)) %>%
  mutate(dist_km = NEAR_DIST/100)

# looks like theres a couple sites where we don't have environmental information. 
# No temperature or salinity for: Ballena Island and South Wadleigh (plus no dist info)
```

```{r}
# avg multiple adj sites that occurred in the same hab
require(viridis) # for fun color palette

fish.avg <- pair.fish %>%
  group_by(site_pair, Habitat, SpCode) %>%
  mutate(avg = mean(abundance), sd = sd(abundance)) %>%
  mutate(avg = replace_na(avg, 0), sd = replace_na(sd, 0))
```
## Graph CPUE by habitat
```{r g1}
g1 <- fish.avg %>%
  group_by(SiteID) %>%
  drop_na(SpCode) %>%
  ggplot() +
  geom_bar(mapping = aes(x = Habitat, y = avg, fill = SpCode), 
           position = "dodge", stat = "identity") +
  geom_errorbar(mapping = aes(x = Habitat, ymax = avg+sd, ymin = avg-sd), 
                width = 0, stat = "identity") +
  xlab("Habitat") + ylab("CPUE") + 
  facet_wrap(~site_pair) + 
  plot_theme() +
  geom_text(mapping = aes(x = Habitat, y = avg + 200, group = SpCode,
                          label = format(avg, digits = 1, scientific = FALSE)),
            colour = "black", position = position_dodge(0.9), hjust = 0.7)
  
g1 + scale_fill_brewer(name = "Species", labels = c("Chum", "Coho", "Pink"), palette = "Dark2")
```

Looking at average CPUE of salmon  in vegetated versus unvegetated sites
```{r}
fish.avg %>%
  mutate(Hab_type = ifelse(Habitat %in% c("Bedrock", "Sand-gravel"), "Unvegetated", "Vegetated")) %>%
  group_by(Hab_type) %>%
  ggplot(mapping = aes(x = Hab_type, y = abundance)) +
  geom_boxplot() +
  xlab("Habitat type") + ylab("CPUE") +
  plot_theme()
```

## Graph avg fish per unveg/veg
```{r}
fish.sum <- fish.avg %>%
  drop_na(SpCode) %>%
  mutate(Hab_type = ifelse(Habitat %in% c("Bedrock", "Sand-gravel"), "Unvegetated", "Vegetated")) %>%
  group_by(Hab_type, SpCode) %>%
  dplyr::summarise(avg = mean(abundance), sd = sd(abundance))

g2 <- ggplot(data = fish.sum, aes(x = Hab_type, y = avg, ymin = avg-sd, ymax = avg+sd, fill = SpCode)) +
  geom_bar(position = position_dodge(), aes(y = avg), stat = "identity") +
  geom_errorbar(position = position_dodge(width = 0.9), colour = "black", width = 0.1) +
  xlab("Habitat") + ylab("CPUE") +
  plot_theme()
g2 + scale_fill_brewer(name = "Species", labels = c("Chum", "Coho", "Pink"), palette = "Dark2")
```

## Catch by lat/long
```{r}
g <- sal_env %>%
  drop_na(SpCode) %>%
  ggplot() +
  geom_point(aes(x = Longitude, y = Latitude, size = abundance, shape = Habitat)) +
  plot_theme()
g
```

## table sites by # seines and date of seine
Want to create a table that includes site pair, habitat type, number seines, and date of seine
```{r}
# read about using knitr::kable and kableExtra package to create a table. lets load it
library(kableExtra)
library(webshot)
webshot::install_phantomjs()

site_table <- sal_env %>%
  dplyr::group_by(site_pair, Habitat, Date) %>%
  dplyr::select(-c(NEAR_DIST, SpCode, abundance, Temp, Salinity, Habitat, 
                   Gear, julian, dist_km, 
                   SubLocale)) %>%
  distinct() %>%
  dplyr::summarise(Seine_no = n()) %>%
  ungroup() %>%
  kbl(col.names = c("Site Location", "Habitat", "Date", "Number of Seines")) %>%
  kable_classic
site_table

#save_kable(site_table, "Data/Chp1Hyp2Table.jpeg")
```

Theres clearly a lot of variability in salmon catches, but at first glance at the graphs make it look like there isn't a large diff in the number of salmon caught in each hab, maybe with more salmon caught in the unvegetated habitat. 
## Assesing distribution/normality
We've done some basic visualizations above, but lets also look at normality, outliers, collinearity, etc. As well as looking into transformations. 
```{r}
# abundance histograms
hist(sal_env$abundance) # lots of zeros and a few large abundances
hist(log(sal_env$abundance)) # looks slightly more normal...
plot(density(sal_env$abundance))


boxplot(abundance ~ SpCode, data = sal_env)
boxplot(abundance ~ Habitat, data = sal_env)
# boxplot by spcode and hab
g3 <- sal_env %>%
  drop_na(SpCode) %>%
  ggplot() +
  geom_boxplot(aes(x = SpCode, y = abundance, fill = Habitat)) +
  plot_theme() +
  ylab("CPUE") + xlab("Species Common Name") +
  scale_x_discrete(breaks = c("SALCHUM", "SALCOHO", "SALPINK"),
                   labels = c("Chum", "Coho", "Pink"))
g3 + scale_fill_brewer(palette = "Dark2")

# check fo normality (based on Mod3 Fish 604 course)
x <- sort(sal_env$abundance)
qqnorm(x) # plot sample points
qqline(x) # plot normality line -- doesn't look normal not along line

# appoximate log-normal distribution
qqnorm(log(x +1)) # fits a lot better than no transformation, still not perf
qqline(log(x +1)) # some skew along the tails

hist(x) # same as above. heavy right skew
hist(log(x +1)) # better but maybe still right skewed

# 4th root 
qqnorm((x^(1/4))) # log looks way better
qqline((x^(1/4)))
```

So while investigating normality, a log transformation appears to most appropriate to appoximate normality in the cpue catch data for salmon. Now another question may be around outliers. It appears that there is one site where over 800 salmon were caught, is this an outlier? What does having the point do to the skew of the data?

## Outliers
```{r}
plot(abundance ~ Habitat, data = sal_env) #point occured in an bedrock habitat
boxplot(abundance ~ SpCode, data = sal_env) # was 800 pink salmon in bedrock
# if you look at the dataset its row 19 at Johnson Cove (Site # 597) that is the potential outlier

plot(abundance ~ Temp, type = "n", data = sal_env)
points(abundance ~ Temp, subset = -19, cex = 1.5, data = sal_env)
points(abundance ~ Temp, subset = 19, pch = 17, data = sal_env) # highlighted pot. outliner in triangle

# In order to easily change which parameter we're looking at lets set up x1
y <- sal_env$abundance
x1 <- sal_env$dist_km

# lets look at the residuals when we fit a linear model with Temp (and other variables?)
fit.ln <- lm(y ~ x1, data = sal_env)
summary(fit.ln)
plot(fit.ln) 

# looking at the residuals, the outlier (case # 19) appears to be an influential outlier. What happens to the residuals etc when we remove it?
sal_env_red <- (sal_env[-19,])
y2 <- sal_env_red$abundance
x2 <- sal_env_red$dist_km

fit.ln.out <- lm(y2 ~ x2)
summary(fit.ln.out)
plot(fit.ln.out)

plot(y ~ x1, type = "n", xlab = "Distance from anadromous stream (km)", ylab = "CPUE")
points(y ~ x1, subset = -19, cex = 1.5)
points(y ~ x1, subset = 19, pch = 17) # highlighted pot. outliner in triangle
abline(fit.ln) # add in linear regression line including outlier
abline(fit.ln.out, lty = 2, col = "red") # linear regression w/o outlier

# lets look at abundance across habitat type w/ and w/o the outlier
boxplot(abundance ~ Habitat, data = sal_env_red)
boxplot(abundance ~ Habitat, data = sal_env)
g3 <- sal_env_red %>%
  drop_na(SpCode) %>%
  ggplot() +
  geom_boxplot(aes(x = SpCode, y = abundance, fill = Habitat)) +
  plot_theme() +
  ylab("CPUE") + xlab("Species Common Name") +
  scale_x_discrete(breaks = c("SALCHUM", "SALCOHO", "SALPINK"),
                   labels = c("Chum", "Coho", "Pink"))
g3 + scale_fill_brewer(palette = "Dark2")
```


Lets use some GLMs to evaluate relationships between salmon catch in habitats

# GLM explorations - total abundance
## Visualizations

```{r data structure}
corrgram(sal_env[,c(8, 9:11, 21, 22)], lower.panel = panel.shade,
         upper.panel = panel.cor, diag.panel = panel.density)

# can we seperate the abundances by species to look at possible species based correlations?
sal_env_wide <- pivot_wider(sal_env, names_from = SpCode, values_from = abundance)

corrgram(sal_env_wide[,c(8,9, 19:23)], lower.panel = panel.shade, 
         upper.panel = panel.cor, diag.panel = panel.density)
# Looks like theres a couple explanatory parameters that shouldn't be included together since they're already strongly correlated. Temperature and Julian day

sal_env$Salinity[sal_env$Salinity == 0] <- NA
sal_env$Temp[sal_env$Temp == 0] <- NA

```

```{r}
# Graph species abundance against distance from anad dist
g <- sal_env %>%
  drop_na(SpCode) %>%
  ggplot(aes(x = dist_km, y = abundance, color = SpCode)) + 
  geom_point(size = 2) + 
  plot_theme() +
  ylab("CPUE") + xlab("Distance from anadromous stream (km)")
g3 <- g + scale_color_brewer(name = "Species", labels = c("Chum", "Coho", "Pink"), palette = "Dark2")
```
```{r}
g4 <- sal_env %>%
  drop_na(SpCode) %>%
  ggplot(aes(x = Temp, y = abundance, color = SpCode)) + 
  geom_point(size = 2) + 
  plot_theme() +
  ylab("CPUE") + xlab("Temperature (C)")
g4 <- g4 + scale_color_brewer(name = "Species", labels = c("Chum", "Coho", "Pink"), palette = "Dark2")
```
```{r}
g5 <- sal_env %>%
  drop_na(SpCode) %>%
  ggplot(aes(x = Salinity, y = abundance, color = SpCode)) + 
  geom_point(size = 2) + 
  plot_theme() +
  ylab("CPUE") + xlab("Salinity")
g5 <- g5 + scale_color_brewer(name = "Species", labels = c("Chum", "Coho", "Pink"), palette = "Dark2")
```
```{r}
g6 <- sal_env %>%
  drop_na(SpCode) %>%
  ggplot(aes(x = julian, y = abundance, color = SpCode)) + 
  geom_point(size = 2) + 
  plot_theme() +
  ylab("CPUE") + xlab("Julian day")
g6 <- g6 + scale_color_brewer(name = "Species", labels = c("Chum", "Coho", "Pink"), palette = "Dark2")
```

Arrange the plots together to look at the same time
```{r}
library(cowplot)
plot_grid(g3, g4, g5, g6,
          labels = c("A", "B", "C", "D"),
          ncol = 2, nrow = 2)
```
## Binomial 
Lets first set up some explorations with binomial models to look at presence absence. 
```{r}
sal_env <- sal_env %>%
  replace(is.na(.), 0) %>%
  mutate(Hab_type = ifelse(Habitat %in% c("Bedrock", "Sand-gravel"), "Unvegetated", "Vegetated"))
hist(as.numeric(sal_env$abundance > 0))

plot((abundance > 0) ~ julian, data = sal_env)
plot((abundance > 0) ~ Temp, data = sal_env)
plot((abundance > 0) ~ Salinity, data = sal_env)
plot((abundance > 0) ~ dist_km, data = sal_env)

# convert to 0 & 1 for binomial model
sal_env$abundance[is.na(sal_env$abundance)] = 0
sal_env$binomial[sal_env$abundance > 0] <- 1
sal_env$binomial[sal_env$abundance == 0] <- 0

```

What is our question? 
For the binomial model, we're hypothesizing that habitat type (specifically the presence of vegetation in eelgrass and kelp habitats) will determine presence of salmon. Whereas unvegetated habitats will not  have fish. 

Will include: julian day(+), salinity (no real correl), distance (-) with habitat types 
In a seperate model include temp (no correl), salinity, and distance with habitat types
Then compare the best model between the two

1. First including julian day rather than temp
```{r}
# 
fit.bn.intx <- glm(binomial ~ I(julian^2) + julian + Habitat * Salinity + dist_km, family = binomial, data = sal_env)
summary(fit.bn.intx) # based on the z-test 'summary' function there is likely no significant interaction. I checked both salinity * distance and julian day * salinity. This can be confirmed with a chi-square test

anova(fit.bn.intx, test="Chisq") # confirmed there is no significant interaction. So only include additive parameters. 

# model w/o intx
fit.bn <- glm(binomial ~ I(julian ^2) + julian + Habitat +Salinity + dist_km, family = binomial, data = sal_env)
summary(fit.bn)

AIC(fit.bn.intx, fit.bn) # keep the model w/o interactions. 

# Okay so now, in terms of our presence/absence models, do we need all these parameters. Lets drop salinity since when graphed against abund there wasn't any real correlation

fit.bn2 <- glm(binomial ~ I(julian ^ 2) + julian + Habitat + dist_km, family = binomial, data = sal_env)
summary(fit.bn2)

# lets drop distance from anadramous stream as well
fit.bn3 <- glm(binomial ~ I(julian ^ 2) + julian + Habitat, family = binomial, data = sal_env)
summary(fit.bn3)

# because there are only really two time frames that habitats were sampled (May and June) lets drop the quadratic for julian day

fit.bn4 <- glm(binomial ~ julian + Habitat, family = binomial, data = sal_env)
summary(fit.bn4) # hmm dropping the quadratic seems to not be as great

fit.bn5 <- glm(binomial ~ Habitat, family = binomial, data = sal_env)
summary(fit.bn5)

# so since these models are *nested* meaning I sequentially dropped parameters, we can compare how well these models fit the data using AIC values. 

AIC(fit.bn, fit.bn2, fit.bn3, fit.bn4, fit.bn5)
# the AIC values are relatively close, but the 3rd and 5th models both have comparitively similar AIC values. At most the change in AIC is around 2 meaning that choosing the most parsimonious model may be a better choice. However, we need to employ our own logic about that system and since we know that salmon do outmigrate during the summer, julian day *should* stay in the model. 
# Lets compare the two using likelihood ratio test
anova(fit.bn5, fit.bn3, test = "LRT")
# the null hypothesis (that the simplier model is the true model) was not rejected, meaning that fit.bn5 (w/o julian) is closer to the true model. 

# However since we are interested in Habitat type and we know julian is important we'll keep both (model fit.bn3) as the true model for visualization. 

visreg(fit.bn3, gg = TRUE, partial = FALSE)

# whats the goodness of fit like? High two lines
par(mfrow=c(2,2))
plot(fit.bn3, which = 1:4) # Theres some issues of fit evidenced by some patterns in data (scale-location). 
```

2. Temperature instead of Julian day
Because they are correlated slight (see pairwise scatterplot from above) we only want to include one or the either in a model. Lets do the same process as above and then use AIC values to test which is better!

```{r}
# Lets test interactions again
fit.bn.intx2 <- glm(binomial ~ Temp * dist_km * Salinity + Habitat, family = binomial, data = sal_env)
summary(fit.bn.intx2) # based on the z-test 'summary' function there is likely no significant interaction. I checked both salinity * distance and julian day * salinity. This can be confirmed with a chi-square test

anova(fit.bn.intx2, test = "Chisq") # confirmed

# Just additive
fit.bn.t <- glm(binomial ~ Temp + dist_km + Salinity + Habitat, family = binomial, data = sal_env)
summary(fit.bn.t)

# Lets start dropping parameters and seeing what it does to the model and its deviance etc. 
# Salinity had no correl
fit.bn.t2 <- glm(binomial ~ Temp + dist_km + Habitat, family = binomial, data = sal_env)
summary(fit.bn.t2)

fit.bn.t3 <- glm(binomial ~ Temp + Habitat, family = binomial, data = sal_env)
summary(fit.bn.t3)

# lets compare AIC values (include model w/ just hab too)
AIC(fit.bn.intx2, fit.bn.t, fit.bn.t2, fit.bn.t3, fit.bn5) # ust hab appears to the better model, second to that is the model that included temp and habitat. Lets compare the temp hab and julian day hab (that we decided was the better model from above)

# comparing the best model from below w/ the best from above
AIC(fit.bn.t3, fit.bn3) # looks like julian day and Hab is a better way to look at the data still
plot(fit.bn.t3, 1:6) # still a weird fit, maybe binomial isn't the best. 
```

Okay so having looked at diff parameters with the binomial logistic model and seeing that dispite figuring out which model is the "better" model, the fit of the model was relatively low. Lets try using a different type of model. 
First lets look at the poisson model, then depending on if the data are overdispersed (common for count data) -- try using a negative binomial or zero-inflated model (test for too many zeros using a vuong test?) 
A poisson distribution uses a *constant* term for variance, i.e. the variance cannot change across group etc. Negative binomial distribution allows for variance to essential *vary* within the model. 

## Poisson distribution
```{r}
# Lets try looking at the log transformation of the data since poisson distribution uses a "log link"

plot(sal_env$julian, log(sal_env$abundance + 1)) # +1 because you can't take the log of 0
plot(sal_env$Temp, log(sal_env$abundance + 1))
plot(sal_env$Salinity, log(sal_env$abundance + 1))
plot(sal_env$dist_km, log(sal_env$abundance + 1))
plot(sal_env$Habitat, log(sal_env$abundance + 1))

# Lets look at it using scatter.smooth using the LOESS smooth function
scatter.smooth(sal_env$julian, log(sal_env$abundance + 1)) # +1 because you can't take the log of 0
scatter.smooth(sal_env$Temp, log(sal_env$abundance + 1))
scatter.smooth(sal_env$Salinity, log(sal_env$abundance + 1))
scatter.smooth(sal_env$dist_km, log(sal_env$abundance + 1))
scatter.smooth(sal_env$Habitat, log(sal_env$abundance + 1))
```

Fit the model
```{r}
# Create two models: one with temp and the other with julian
# Start with julian
fit.pois <- glm(abundance ~ julian + I(julian ^2) + Salinity + dist_km + Habitat, data = sal_env, family = poisson)
summary(fit.pois) # why are the significance so diff that the binomial above? 

# Can use a pearson goodness of fit test to look at if the poisson distribution model is a good fit for the data
# Extract the pearson residuals (residuals(type = "pearson")), square them and then take the sum: 
chisq <- sum(residuals(fit.pois, type="pearson")^2)
# The chisq value should follow a x2(chisquare) distribution, if the null hypothesis is true (i.e. the data follow a poisson distribution)

pchisq(chisq, df.residual(fit.pois), lower.tail = FALSE)# this is supposed to be a p-value that tells us if we reject the null. With a pvalue of ~ 0 we reject the null hypothesis that this fits the poisson distribution. 
pchisq(deviance(fit.pois), df.residual(fit.pois), lower.tail = FALSE)  # can do a similar test using the deviance. 

# this may be because of overdispersion (i.e. more variance than accounted for in the model) To check for overdispersion you need to look at the residuals of the model

r <- residuals(fit.pois)
y.hat <- fitted(fit.pois)
par(mfrow=c(1,1))
plot(y.hat,r)
abline(h=c(-2,0,2),col=c(3,2,3))

deviance(fit.pois)/df.residual(fit.pois) 

#' The overdispersion parameter is larger than one (84.8). Wayy larger, clearly there is overdispersion
```

## Quasipoisson
A quasi-likelihood model permits extra variation by multiplying the variance term by an appropriate “adjustment factor”. The fitted values will be identical to those from the Poisson log-linear regression, but the standard errors will be larger, and hence the p-values will be larger as well. In R, a quasi-likelihood model for tbe Poisson can be fit by using family=quasipoisson instead of family=poisson in the call to 'glm()'
```{r}
fit.quasi <- update(fit.pois, family = quasipoisson)
summary(fit.quasi) # overdispersion 138, estimates are the same, but SE seem to be bigger
summary(fit.pois)

# Does the quasipoisson distribution look right
par(mfrow=c(2,3))
plot(fit.quasi, which=1:6)
# possible influential outlier (case # 19). The pearson residuals vary across the predicted values indicating that they are *not* randomly distributed. Lack of homogeneity in residuals. Curious what would happen to fit if you removed the outlier
fit.quasi.out <- update(fit.quasi, data = sal_env[-19,])
summary(fit.quasi.out)

par(mfrow = c(2,3))
plot(fit.quasi.out, which = 1:6) # maybe looks slightly better but similar issues still remain. 
```

## Negative binomial 
Another approach to overdispersion (like we saw above) is using the negative binomial distribution. However, this will not be able to deal with the heterozygosity in the residuals (but I dont think that negative binomial assumes constant variance?)

1. try w/ julian day but also next with temp
```{r}
library(MASS)
# note if you want to use select from dplyr, use dplyr::select
fit.nb <- glm.nb(abundance ~ julian + I(julian ^2) + Salinity + dist_km + Habitat, data = sal_env)
summary(fit.nb)

library(lmtest)
lrtest(fit.pois, fit.nb) # null hypothesis is reject the same model with a neg binomial distribution is considered the "better model"

# lets create iterations of the model with fewer non significant parameters
# already checked interactions between parameters, eithe rth emodel doesn't converge if all are considereed interactions or are non-significant 
fit.nb2 <- glm.nb(abundance ~ julian + dist_km + Salinity + Habitat, data = sal_env)
summary(fit.nb2)

fit.nb3 <- glm.nb(abundance ~ julian + I(julian ^2) + Salinity + Habitat, data = sal_env)
summary(fit.nb3)

fit.nb4 <- glm.nb(abundance ~ julian + I(julian ^2) + Habitat, data = sal_env)
summary(fit.nb4)

fit.nb5 <- glm.nb(abundance ~ julian + dist_km + Habitat, data = sal_env)
summary(fit.nb5)

fit.nb6 <- glm.nb(abundance ~ julian + Salinity + Habitat, data = sal_env)
summary(fit.nb6)

fit.nb7 <- glm.nb(abundance ~ julian + Habitat, data = sal_env)
summary(fit.nb7)

# Lets compare models 
AIC(fit.nb, fit.nb2, fit.nb3, fit.nb4, fit.nb5, fit.nb6, fit.nb7) 
# at most theres only a change of ~3 of the AIC value, but the models that are better are the fit.nb 6 and fit.nb7 -- lets compare them

anova(fit.nb6, fit.nb7)
lrtest(fit.nb6, fit.nb7)
# Does not reject the null (pvalue = 0.1), so the simplier model is the "better model" 

# Lets look at the fit of this model that includes julian day and Habitat
par(mfrow = c(2,3))
plot(fit.nb6, which = 1:6) # the qqplot indicates that there may be some right or positive skew of the data. Isn't essential for interpretating the data (I think) but should be noted. The residuals against the predicted values are... better but there are some patterns. 
```

Lets visualize this model 'fit.nb6' that we've determined is the better model. 

```{r}
par(mfrow = c(1,1))
visreg(fit.nb6) # all graphs
# by each parameter

h <- visreg(fit.nb6, "Habitat", scale = "response", gg = TRUE, partial = TRUE, alpha = .05,
            line = list(col = "black")) +
  plot_theme() + 
  coord_cartesian(ylim = c(0, 700)) +
  ylab("CPUE")

j <- visreg(fit.nb6, "julian", scale = "response", gg = TRUE, partial = TRUE, alpha = .05,
            line = list(col = "black")) + 
  coord_cartesian(ylim = c(0, 700)) +
  plot_theme() + 
  ylab("CPUE") +
  xlab("Julian Day")

s <- visreg(fit.nb6, "Salinity", scale = "response", gg = TRUE, partial = TRUE, alpha = .05, 
            line = list(col = "black")) + 
  coord_cartesian(ylim = c(0, 700)) +
  plot_theme() + 
  ylab("CPUE")
  

plot_grid(h, j, s,
          labels = c("A", "B", "C"),
          ncol = 2, nrow = 2)
```

We know that each species of salmon have different life histories, so they might be influenced by different parameters than total salmon abundance. Lets start with our most abundant species--pink salmon

# GLM explorations - pink salmon 
```{r pink}
pk <- sal_env %>%
  filter(SpCode == "SALPINK")

pink <- sal_env %>%
  dplyr::select(-c(SpCode, abundance, binomial)) %>%
  distinct() %>%
  left_join(pk)

pink$abundance[is.na(pink$abundance)] = 0
pink <- pink %>%
  mutate(log_abund = log(abundance + 1)) # lets create a column that has a log transformed abundance and refit some of the models with this column to look at how it compares

# but somehow the temperature data that was NA has been mistakeningly changed to 0, change back
pink$Temp[pink$Temp == 0] <- NA
pink$Salinity[pink$Salinity == 0] <- NA
```

## Visualizations
```{r}
corrgram(pink[,c(8:10, 19, 20, 23, 25)], lower.panel = panel.shade,
         upper.panel = panel.cor, diag.panel = panel.density)

# Looks like theres a couple explanatory parameters that shouldn't be included together since they're already strongly correlated. Temperature and Julian day
# But also loooks like salinity and dist are correlated
```

```{r}
p1 <- pink %>%
  drop_na(SpCode) %>%
  ggplot(aes(x = dist_km, y = abundance, color = Habitat)) + 
  geom_point(size = 2) + 
  plot_theme() +
  ylab("CPUE") + xlab("Distance from anadromous stream (km)")
p1 + scale_color_brewer(palette = "Dark2")
```
```{r}
p2 <- pink %>%
  drop_na(SpCode) %>%
  ggplot(aes(x = julian, y = abundance, color = Habitat)) + 
  geom_point(size = 2) + 
  plot_theme() +
  ylab("CPUE") + xlab("Julian Day")
p2 + scale_color_brewer(palette = "Dark2")
```
```{r}
p3 <- pink %>%
  drop_na(SpCode) %>%
  ggplot(aes(x = Salinity, y = abundace, color = Habitat)) + 
  geom_point(size = 2) + 
  plot_theme() +
  ylab("CPUE") + xlab("Salinity")
p3 + scale_color_brewer(palette = "Dark2")
```
```{r}
p4 <- pink %>%
  drop_na(SpCode) %>%
  ggplot(aes(x = Temp, y = abundace, color = Habitat)) + 
  geom_point(size = 2) + 
  plot_theme() +
  ylab("CPUE") + xlab("Temperature (C)")
p4 + scale_color_brewer(palette = "Dark2")
```

```{r}
plot_grid(p1, p2, p3, p4,
          labels = c("A", "B", "C", "D"),
          ncol = 2, nrow = 2)
```

Against habitat?
```{r}
pink %>%
  ggplot() +
  geom_boxplot(aes(x = Habitat, y = abundance)) +
  plot_theme() +
  ylab("Pink salmon CPUE")

# out of curiorsiity does this habitat pattern happen across all seines in seak that happened in may and june? 

seak_sal %>%
  filter(Mon == "May" | Mon == "Jun") %>%
  ggplot() +
  geom_boxplot(aes(x = Habitat, y = abundance, color = SpCode)) +
  plot_theme()

```

## Binomial model
Wasn't the greatest model for overall total abundance, but worthwhile to go through the steps for pinks as well. 

```{r}
# set up binomial make sure columns are right. 
glimpse(pink) 
# change NAs in binomial column to zeros
pink$binomial[is.na(pink$binomial)] = 0
unique(pink$binomial) # check! 
```

Our question for binomial model:
For the binomial model, we're hypothesizing that habitat type (specifically the presence of vegetation in eelgrass and kelp habitats) will determine presence of salmon. Whereas unvegetated habitats will not  have fish. 

Based on the corrgram done above, temperature and julian day are correlated (+) and Salinity and distance from anadromous stream (-) at around r = 0.7 might be best to keep those in seperate models. 
1. Binomial models with julian, dist, habitat
```{r}
# check for interactions
pink.intx <- glm(binomial ~  julian * I(julian ^2) + dist_km * Habitat, data = pink, family = binomial)
summary(pink.intx) # cant have all intx cause convergence doesn't happen. 

# remove interactions
pink.b <- glm(binomial ~ julian + I(julian^2) + dist_km + Habitat, data = pink, family = binomial)
summary(pink.b)

# drop dist_km
pink.b2 <- glm(binomial ~ julian + I(julian^2) + Habitat, data = pink, family = binomial)
summary(pink.b2)

# drop julian^2
pink.b3 <- glm(binomial ~ julian + Habitat, data = pink, family = binomial)
summary(pink.b3)

# julian
pink.b4 <- glm(binomial ~ Habitat, data = pink, family = binomial)
summary(pink.b4) # not the most ideal model cause we know julian is impt

AIC(pink.intx, pink.b, pink.b3, pink.b4)
```

2. Binomial models with julian, salinity, habitat
```{r}
# check for interactions
pink.intx2 <- glm(binomial ~  Salinity * I(julian ^2) + julian * Habitat, data = pink, family = binomial)
summary(pink.intx2) # cant have all intx cause convergence doesn't happen. 

# remove interactions
pink.b5 <- glm(binomial ~ julian + I(julian^2) + Salinity + Habitat, data = pink, family = binomial)
summary(pink.b5)

# other versions of the model with Salinity are the same as the ones done above.
AIC(pink.intx2, pink.b5)
```

3. Binomial models with temp, dist, habitat
```{r}
# check for interactions
pink.intx3 <- glm(binomial ~ dist_km + Temp * Habitat, data = pink, family = binomial)
summary(pink.intx3)

# only addiative
pink.b6 <- glm(binomial ~ dist_km + Temp + Habitat, data = pink, family = binomial)
summary(pink.b6)

# drop dist
pink.b7 <- glm(binomial ~ Temp + Habitat, data = pink, family = binomial)
summary(pink.b7)

# drop temp instead
pink.b8 <- glm(binomial ~ dist_km + Habitat, data = pink, family = binomial)
summary(pink.b8)

# just for funnsies
pink.b9 <- glm(binomial ~ Habitat, data = pink, family = binomial)
summary(pink.b9)

AIC(pink.intx3, pink.b6, pink.b7, pink.b8, pink.b9)
```

4. Binomial models with temp, salinity, habitat

```{r}
# full model with interactions
pink.intx4 <- glm(binomial ~  Salinity + Temp * Habitat, data = pink, family = binomial)
summary(pink.intx4)

pink.b10 <- glm(binomial ~ Temp + Salinity + Habitat, data = pink, family = binomial)
summary(pink.b10)

pink.b11 <- glm(binomial ~ Salinity + Habitat, data = pink, family = binomial)
summary(pink.b11)

AIC(pink.intx4, pink.b10, pink.b11)
```

Okay which are the best models?
```{r}
AIC(pink.b11, pink.b7, pink.b9, pink.b5, pink.b, pink.b4)
# they are all very similar but the three that are the lowest AIC values are:
# pink.b, pink.b5, pink.b4

summary(pink.b)
summary(pink.b5)
summary(pink.b4) # having just habitat isnt ideal cause we know julian day is impt

AIC(pink.b, pink.b5) # pink.b including julian day and dist and hab is the lowest AIC and likely the "better" model. 
```

Okay so if the "better" model is 'pink.b', lets look at the fit of the model and visualize the results
```{r}
visreg(pink.b, scale = "response")
# thoughts about julian being more of a categorical variable rather than continuous?
# how does the model fit?
par(mfrow = c(2,3))
plot(pink.b, which = 1:6) # uff not that well.. looks like some patters in the residuals. 
```

## Poisson distribution
Visualizing the log distribution
```{r}
# Lets try looking at the log transformation of the data since poisson distribution uses a "log link"

plot(pink$julian, log(pink$abundance + 1)) # +1 because you can't take the log of 0
plot(pink$Temp, log(pink$abundance + 1))
plot(pink$Salinity, log(pink$abundance + 1))
plot(pink$dist_km, log(pink$abundance + 1))
plot(pink$Habitat, log(pink$abundance + 1))

# Lets look at it using scatter.smooth using the LOESS smooth function
scatter.smooth(pink$julian, log(pink$abundance + 1)) # +1 because you can't take the log of 0
scatter.smooth(pink$Temp, log(pink$abundance + 1))
scatter.smooth(pink$Salinity, log(pink$abundance + 1))
scatter.smooth(pink$dist_km, log(pink$abundance + 1))
scatter.smooth(pink$Habitat, log(pink$abundance + 1))
```


```{r}
# Create two models: one with temp and the other with julian
# Start with julian
pois.pink <- glm(abundance ~ julian + I(julian ^2) + dist_km + Habitat, data = pink, family = poisson)
summary(pois.pink) # why are the significance so diff that the binomial above? 

# Can use a pearson goodness of fit test to look at if the poisson distribution model is a good fit for the data
# Extract the pearson residuals (residuals(type = "pearson")), square them and then take the sum: 
chisq <- sum(residuals(pois.pink, type="pearson")^2)
# The chisq value should follow a x2(chisquare) distribution, if the null hypothesis is true (i.e. the data follow a poisson distribution)

pchisq(chisq, df.residual(pois.pink), lower.tail = FALSE)# this is supposed to be a p-value that tells us if we reject the null. With a pvalue of ~ 0 we reject the null hypothesis that this fits the poisson distribution. 
pchisq(deviance(pois.pink), df.residual(pois.pink), lower.tail = FALSE)  # can do a similar test using the deviance. 

# this may be because of overdispersion (i.e. more variance than accounted for in the model) To check for overdispersion you need to look at the residuals of the model

r <- residuals(pois.pink)
y.hat <- fitted(pois.pink)
par(mfrow=c(1,1))
plot(y.hat,r)
abline(h=c(-2,0,2),col=c(3,2,3))

deviance(pois.pink)/df.residual(pois.pink) 

#' The overdispersion parameter is larger than one (123.8). Wayy larger, clearly there is overdispersion
```


## Quasipoisson
```{r}
quasi.pink <- update(pois.pink, family = quasipoisson)
summary(quasi.pink) # overdispersion 138, estimates are the same, but SE seem to be bigger
summary(pois.pink)

# Does the quasipoisson distribution look right
par(mfrow=c(2,3))
plot(quasi.pink, which=1:6)
# possible influential outlier (case # 10). The pearson residuals vary across the predicted values--linear trend-- indicating that they are *not* randomly distributed. Lack of homogeneity in residuals. Curious what would happen to fit if you removed the outlier
quasi.pink.out <- update(quasi.pink, data = sal_env[-c(10),])
summary(quasi.pink.out) #nothing really in the summary, might improve sign of eelgrass...

par(mfrow = c(2,3))
plot(quasi.pink.out, which = 1:6)
```

## Negative binomial 
Another approach to overdispersion (like we saw above) is using the negative binomial distribution. However, this will not be able to deal with the heterozygosity in the residuals (but I dont think that negative binomial assumes constant variance?)

1. Neg bi models with julian, dist, habitat
```{r}
pink.nb <- glm.nb(abundance ~ julian + I(julian^2) + dist_km + Habitat, data = pink, init.theta=0.5, control = glm.control(maxit = 10000))
summary(pink.nb)

# drop the julian2 
pink.nb2 <- glm.nb(abundance ~ julian + dist_km + Habitat, data = pink)
summary(pink.nb2)

# bring back j2 and drop dist
pink.nb3 <- glm.nb(abundance ~ julian + I(julian^2) + Habitat, data = pink, init.theta = 0.5, 
                   control = glm.control(maxit = 1000))
summary(pink.nb3)

# drop julian2
pink.nb4 <- glm.nb(abundance ~ julian + Habitat, data = pink)
summary(pink.nb4)

# drop julian (but we know its impt)
pink.nb5 <- glm.nb(abundance ~ Habitat, data = pink)
summary(pink.nb5)

# because these models are nested, can use likelihood ratio test
lrtest(pink.nb, pink.nb2) # null hypothesis true, dropping jul2 justified
lrtest(pink.nb, pink.nb3) # null hypothesis true, droopping dist justified
lrtest(pink.nb, pink.nb4) # null hypothesis true
lrtest(pink.nb, pink.nb5) # null hypo true, HOWEVER. We know that julian is an impt covariate
lrtest(pink.nb4, pink.nb5) # pvalue 0.06 -- rejected at 0.1
AIC(pink.nb4, pink.nb5)
AICc(pink.nb4);AICc(pink.nb5)
# pink.nb4 is the better of these 
```

2. Neg bi models with julian, salinity, habitat
```{r}
# repeat with salinity instead of dist

pink.nb6 <- glm.nb(abundance ~ julian + I(julian ^ 2) + Salinity + Habitat, data = pink,
                   init.theta = 0.5, control = glm.control(maxit = 100))
summary(pink.nb6)

#drop jul2
pink.nb7 <- glm.nb(abundance ~ julian + Salinity + Habitat, data = pink) 
summary(pink.nb7)

lrtest(pink.nb6, pink.nb7) # justified in dropping julian2
# can't directly compare pink.nb7 and pink.nb4 because they were not fitted with the same number of observations (had to drop 3 seines were no salinity measures were taken)

# lets create a new model with the reduced dataset
pink.nb4_reduced <- update(pink.nb4, data = pink[-c(5:7),])

lrtest(pink.nb4_reduced, pink.nb7) # null hypothesis true, justified in dropping salinity
AICc(pink.nb4_reduced); AICc(pink.nb7)
```

3. Neg bi models with temp, dist, habitat
```{r}
pink.nb8 <- glm.nb(abundance ~ Temp + dist_km + Habitat, data = pink)
summary(pink.nb8)

pink.nb9 <- glm.nb(abundance ~ Temp + Habitat, data = pink)
summary(pink.nb9)

pink.nb10 <- glm.nb(abundance ~ dist_km + Habitat, data = pink)
summary(pink.nb10)

# because these are nested models can compare with lrtest
lrtest(pink.nb8, pink.nb9) # null hypo true, justified in dropping dist
lrtest(pink.nb8, pink.nb10) # can't compare not the same size dataset, update pink.nb10

pink.nb10_reduced <- update(pink.nb10, data = pink[-c(5:7, 28),], init.theta = 0.5, 
                           control = glm.control(maxit = 1000))

lrtest(pink.nb8, pink.nb10_reduced) # null hypothesis rejected, keep TEMP
AICc(pink.nb8); AICc(pink.nb10_reduced); AICc(pink.nb9)
# best model is pink.nb9
```

4. Neg bi models with temp, salinity, habitat

```{r}
pink.nb11 <- glm.nb(abundance ~ Temp + Salinity + Habitat, data = pink, init.theta = 0.5, control = glm.control(maxit = 100))
summary(pink.nb11)

pink.nb12 <- glm.nb(abundance ~ Salinity + Habitat, data = pink)
summary(pink.nb12)

pink.nb12_reduced <- update(pink.nb12, data = pink[-c(28),])

lrtest(pink.nb11, pink.nb12_reduced) # null hypothesis rejected, keep TEMP
lrtest(pink.nb11, pink.nb9) # null hypothesis rejected, keep Salinity

AIC(pink.nb11, pink.nb12_reduced, pink.nb9)
AICc(pink.nb11); AICc(pink.nb12_reduced); AICc(pink.nb9)
# The model that includes temperature and salinity is the "better" MODEL (pink.nb11)
```

Compare the best models
```{r}
# pink.nb11 - abund~Temp+Salinity+Habitat
# pink.nb9 - abund~Temp+Habitat
# pink.nb4 - abund~julian + Habitat
AIC(pink.nb11, pink.nb9)
AICc(pink.nb11); AICc(pink.nb9)
# to include pink.nb4 have to reduce observations
pink.nb4_red2 <- update(pink.nb4, data = pink[-c(5:7, 28),])
AIC(pink.nb11, pink.nb9, pink.nb4_red2)

# best model is that that include Temp and Salinity
lrtest(pink.nb9, pink.nb11) # they're nested models
# justified in keep Salinity


# for funsies lets look at what the AICc 
library(AICcmodavg)
AICc(pink.nb11); AICc(pink.nb9); AICc(pink.nb4_red2)
```

Lets look at diagnostics foor both the models
```{r}
plot(pink.nb4, which = 1:6)

plot(pink.nb11, which = 1:6)

# Case number 15 looks like it may be more influential in the pink.nb11 model so lets drop it and see what happens

pink.nb11.upd <- update(pink.nb11, data = pink[-15,]) # whoa definitely lowers AIC value
summary(pink.nb11.upd) # lowers as well but not as much

plot(pink.nb11.upd, which = 1:6)
```

Visualize pink abund models 
```{r}
visreg(pink.nb11)
visreg(pink.nb11.upd)
visreg(pink.nb4)

```
### Visualize neg binomial pink salmon models
Make them look nice and go by model
1. Pink.nb11
```{r}
pink1 <- visreg(pink.nb11, "Temp", scale = "response", gg = TRUE, partial = TRUE, alpha = .05, 
                line = list(col = "black")) + 
  plot_theme() +
  xlab("Temperature (C)") + ylab("Pink CPUE") + 
  geom_point() + coord_cartesian(ylim = c(0, 450))
pink1

pink2 <- visreg(pink.nb11, "Habitat", scale = "response", gg = TRUE, partial = TRUE, alpha = 0.5, 
                line = list(col = "black")) + 
  plot_theme() + coord_cartesian(ylim = c(0, 450)) +
  xlab("Habitat") + ylab("Pink CPUE")
pink2

pink3 <- visreg(pink.nb11, "Salinity", scale = "response", gg = TRUE, partial = TRUE, alpha = 0.5, 
                line = list(col = "black")) + 
  geom_point() + coord_cartesian(ylim = c(0, 450)) +
  plot_theme() + 
  xlab("Salinity") + ylab("Pink CPUE")
pink3

```

visualize neg binomial models
```{r}
plot_grid(pink1, pink2, pink3,
          labels = c("A", "B", "C"),
          ncol = 2, nrow = 2)
```

lets visualize the model with julian day too
```{r}
#pink.nb4
pink4 <- visreg(pink.nb4, "julian", scale = "response", gg = TRUE, partial = TRUE, alpha = .05, 
                line = list(col = "black")) + 
  plot_theme() +
  xlab("Julian Day") + ylab("Pink CPUE") + 
  geom_point() + coord_cartesian(ylim = c(0, 400))
pink4

pink5 <- visreg(pink.nb4, "Habitat", scale = "response", gg = TRUE, partial = TRUE, alpha = 0.5, 
                line = list(col = "black")) + 
  plot_theme() + coord_cartesian(ylim = c(0, 400)) +
  xlab("Habitat") + ylab("Pink CPUE")
pink5

plot_grid(pink4, pink5, 
          labels = c("A", "B"), 
          ncol = 2, nrow = 1)
```

### log transformed neg binomial pink models
Continuing to do things for fun, lets look at logtransformations of the 3 best models
```{r}
pink.nb9
pink.nb9.log <- glm(log_abund ~ Temp + Habitat, data = pink)
summary(pink.nb9.log)

summary(pink.nb11)
pink.nb11.log <- glm(log_abund ~ Temp + Salinity + Habitat, data = pink)
summary(pink.nb11.log)

pink.test.log <- glm(log_abund ~ Salinity + Habitat, data = pink[-c(28),])
summary(pink.test.log)

pink.nb4
pink.nb4.log <- glm(log_abund ~ julian + Habitat, data = pink)
summary(pink.nb4.log)

lrtest(pink.nb9.log, pink.nb11.log) # null hypothesis reject, keep Salinity
AICc(pink.nb9.log); AICc(pink.nb11.log); AICc(pink.nb4.log)
# With log transformation the first two models appear to both have lowerAIC values than the third. But based on the lrtest we SHOULD keep salinity in the model, should we keep temp?
lrtest(pink.nb11.log, pink.test.log) # reject null, keep Temp
# bestmodel (pink.nb11.log)

# visualize briefly
par(mfrow=c(1,3))
visreg(pink.nb11.log)
# fit of model?
par(mfrow=c(2,3))
plot(pink.nb11.log, which = 1:6) # looks like 13 is maybe an influential outlier, for funsies lets drop it
p11.out <- update(pink.nb11.log, data = pink[-13,])
summary(p11.out)
par(mfrow = c(2,3))
plot(p11.out, which = 1:6)

# lets do more diagnositics
par(mfrow = c(2,3))
d1 <- plot(pink[-c(5:7, 28),]$Temp, resid(pink.nb11))
d3 <- plot(pink[-c(5:7, 28),]$Salinity, resid(pink.nb11))
d5 <- plot(pink[-c(5:7, 28),]$Habitat, resid(pink.nb11))

d2 <- plot(pink[-c(5:7, 28),]$Temp, resid(pink.nb11.log))
d4 <- plot(pink[-c(5:7, 28),]$Salinity, resid(pink.nb11.log))
d6 <- plot(pink[-c(5:7, 28),]$Habitat, resid(pink.nb11.log))


# visualize more in depth
pink7 <- visreg(pink.nb11, "Temp", scale = "response", gg = TRUE, partial = TRUE, alpha = .05, 
                line = list(col = "black")) + 
  plot_theme() +
  xlab("Temperature (C)") + ylab("Pink CPUE") + 
  geom_point() + coord_cartesian(ylim = c(0, 450))
pink7

pink8 <- visreg(pink.nb11, "Habitat", scale = "response", gg = TRUE, partial = TRUE, alpha = 0.5, 
                line = list(col = "black")) + 
  plot_theme() + coord_cartesian(ylim = c(0, 450)) +
  xlab("Habitat") + ylab("Pink CPUE")
pink8

pink9 <- visreg(pink.nb11, "Salinity", scale = "response", gg = TRUE, partial = TRUE, alpha = 0.5, 
                line = list(col = "black")) + 
  geom_point() + coord_cartesian(ylim = c(0, 450)) +
  plot_theme() + 
  xlab("Salinity") + ylab("Pink CPUE")
pink9

```

visualize neg binomial models
```{r}
plot_grid(pink7, pink8, pink9,
          labels = ("Originally log-transformed"),
          ncol = 2, nrow = 2)
```

Okay so I noticed that there is some confounding between julian day and location (and likely some with temp too because julian day and temp are correlated (r ~ .7))
Lets try adding in a smoothed parameter with lat and long to account for location in our models using a gam

# GAM exploration - pink salmon
GAM models allows for different error distributions (so family distributions) and link functions. 
incorporates smoothed nonlinear relationships
## negative binomial 

```{r pink gam}
library(mgcv)
library(maps)
library(mapdata)
p.gam <- gam(log_abund ~ s(Longitude, Latitude, bs = "ts", k = 4) + Temp + Salinity + Habitat, data = pink, family = nb(link = "log"))

summary(p.gam)

# lets do some diagnostic visualization
par(mfrow=c(2,3))
plot(pink[-c(5:7, 28),]$Temp, resid(p.gam))
abline(h =c(0, 1, -1), lty =c(1, 2, 2))
plot(pink[-c(5:7, 28),]$Salinity, resid(p.gam))
abline(h =c(0, 1, -1), lty =c(1, 2, 2))
plot(pink[-c(5:7, 28),]$Habitat, resid(p.gam))
plot(pink[-c(5:7, 28),]$Latitude, resid(p.gam))
abline(h =c(0, 1, -1), lty =c(1, 2, 2))
plot(pink[-c(5:7, 28),]$Longitude, resid(p.gam))
abline(h =c(0, 1, -1), lty =c(1, 2, 2))

par(mfrow =c(1,1))
vis.gam(p.gam, c("Longitude", "Latitude"), plot.type =
"contour", color="topo") # does this even mamke sense? 

map('worldHires',fill=T,xlim=c(-140,-130),
ylim=c(54,56),add=T)

# this is supposed to be predicted cpue of pink salmon based on the data. But like further north (where there are no seines) is where its indicating there is an increase in salmon. Clearly the scales are off here. 

p.gam2 <- gam(log_abund ~ s(Longitude, Latitude, bs = "ts", k =4) + Temp + Habitat, data = pink, family = nb(link = "log"))

summary(p.gam2)
par(mfrow = c(1,1))
plot(p.gam)

vis.gam(p.gam2, c("Longitude", "Latitude"), plot.type =
"contour", color="topo") # still doesn't make sense....

map('worldHires',fill=T,xlim=c(-140,-130),
ylim=c(54,56),add=T)

anova(p.gam, p.gam2, test = "Chisq") # null hypothesis rejected at 0.1.... 

p.gam3 <- gam(log_abund ~ s(Longitude, Latitude, bs = "ts", k =4) + Salinity + Habitat, data = pink[-28,], family = nb(link = "log"))
summary(p.gam3) # strange changes to significance

anova(p.gam, p.gam3, test = "Chisq") # null hypothesis, keep temp

p.gam4 <- gam(log_abund ~ Salinity + Temp + Habitat, data = pink, family = nb(link = "log"))
summary(p.gam4)

anova(p.gam, p.gam4, test = "Chisq") # null hypothesis rejected at 0.1
# if we stay with alpha at 0.5 than the ideal gam model includes dropping both salinity and lat/long
```

# GLM explorations - chum salmon

```{r chum}
ch <- sal_env %>%
  filter(SpCode == "SALCHUM")

chum <- sal_env %>%
  dplyr::select(-c(SpCode, abundance, binomial)) %>%
  distinct() %>%
  left_join(ch)

chum$abundance[is.na(chum$abundance)] = 0
chum$binomial[is.na(chum$binomial)] = 0
chum <- chum %>%
  mutate(log_abund = log(abundance + 1)) # lets create a column that has a log transformed abundance and refit some of the models with this column to look at how it compares

# but somehow the temperature data that was NA has been mistakeningly changed to 0, change back
chum$Temp[chum$Temp == 0] <- NA
chum$Salinity[chum$Salinity == 0] <- NA
```

##Visualizations
```{r}
corrgram(chum[,c(8:10, 19, 20, 23, 25)], lower.panel = panel.shade,
         upper.panel = panel.cor, diag.panel = panel.density)

# similar patterns as above (again) -- shouldn't include julian and temp in the same model
```

Visualize the diff parameters against abundance
```{r}
ch1 <- chum %>%
  drop_na(SpCode) %>%
  ggplot(aes(x = dist_km, y = abundance, color = Habitat)) + 
  geom_point(size = 2) + 
  plot_theme() +
  ylab("Chumm salmon CPUE") + xlab("Distance from anadromous stream (km)")
ch1 + scale_color_brewer(palette = "Dark2")

ch2 <- chum %>%
  drop_na(SpCode) %>%
  ggplot(aes(x = julian, y = abundance, color = Habitat)) + 
  geom_point(size = 2) + 
  plot_theme() +
  ylab("Chum salmon CPUE") + xlab("Julian Day")
ch2 + scale_color_brewer(palette = "Dark2")

ch3 <- chum %>%
  drop_na(SpCode) %>%
  ggplot(aes(x = Temp, y = abundance, color = Habitat)) + 
  geom_point(size = 2) + 
  plot_theme() +
  ylab("Chum salmon CPUE") + xlab("Temperature")
ch3 + scale_color_brewer(palette = "Dark2")

ch4 <- chum %>%
  drop_na(SpCode) %>%
  ggplot(aes(x = Salinity, y = abundance, color = Habitat)) + 
  geom_point(size = 2) + 
  plot_theme() +
  ylab("Chum salmon CPUE") + xlab("Salinity")
ch4 + scale_color_brewer(palette = "Dark2")

ch5 <- chum %>%
  drop_na(SpCode) %>%
  ggplot(aes(x = Habitat, y = abundance)) + 
  geom_boxplot() + 
  plot_theme() +
  ylab("Chum salmon CPUE") + xlab("Habitat")
ch5
```

Plot together
```{r}
plot_grid(ch1, ch2, ch3, ch4,
          labels = c("A", "B", "C", "D"),
          ncol = 2, nrow = 2)
```

## Normality?
```{r}
hist(chum$abundance)
hist(chum$log_abund)

hist(sal_env$abundance)
hist(log(sal_env$abundance)) # this isn't the correct way of doing this because log(0) is -inf and that get drops off when you plot it, to look at log(abund) need + constant
hist(log(sal_env$abundance +1))

hist(((chum$abundance)^(1/4))) # doesn't fix the problem of skew

# QQ plots
qqnorm(chum$abundance)
qqline(chum$abundance) # concave shape, tons of spread and trailing of tails

qqnorm(chum$log_abund)
qqline(chum$log_abund) # what is happening at the low sample quartiles?

qqnorm((chum$abundance) ^ (1/4))
qqline((chum$abundance) ^ (1/4)) # same issue. Is the sample size too small?
```

Log transformation may be our best bet when it comes to the chum salmon abundances, but it is still somewhat funcky and has some spread on the lower tail. 

## Binomial model
Lets first look at a presence/absence model using the chum data

Our question for binomial model:
For the binomial model, we're hypothesizing that habitat type (specifically the presence of vegetation in eelgrass and kelp habitats) will determine presence of chum salmon. Whereas unvegetated habitats will not  have fish. 

Based on the corrgram done above, temperature and julian day are correlated (+), might be best to keep those in seperate models. 

1. julian day + dist + salinity + habitat
Unlike the other models, salinity and dist aren't correlated
```{r}
ch.1 <- glm(binomial ~ julian + I(julian^2) + Salinity + dist_km + Habitat, 
            data = chum, family = binomial)
summary(ch.1)

ch.2 <- glm(binomial ~ julian + Salinity + dist_km + Habitat, data = chum, family = binomial)
summary(ch.2)

anova(ch.1, ch.2, test = "Chisq") # null hypothesis accept, drop julian2

ch.3 <- glm(binomial ~ julian + Salinity + Habitat, data = chum, family = binomial)
summary(ch.3)

anova(ch.2, ch.3, test = "Chisq") # null accepted drop dist

ch.4 <- glm(binomial ~ julian + Habitat, data = chum, family = binomial)
summary(ch.4)

ch.4.up <- glm(binomial ~ julian + Habitat, data = chum[-c(5:7),], family = binomial)
summary(ch.4.up)

anova(ch.3, ch.4.up, test = "Chisq") #null accepted drop salinity

ch.5 <- glm(binomial ~ Habitat, data = chum, family = binomial)
summary(ch.5)

anova(ch.4, ch.5, test = "Chisq") # null accept drop julian?

AICc(ch.1);AICc(ch.2);AICc(ch.3);AICc(ch.4);AICc(ch.5)

# based on AIC values the third model (ch.3) with julian salinity and habitat (more than 3 diff in values from nearest [chi.5])
```

2. Binomial models: Temp + salinity + dist + habitat
```{r}
ch.6 <- glm(binomial ~ Temp + Salinity + dist_km + Habitat, data = chum, 
            family = binomial)
summary(ch.6)

ch.7 <- glm(binomial ~ Temp + Salinity + Habitat, data = chum, family = binomial)
summary(ch.7)

anova(ch.6, ch.7, test = "Chisq") # null accepted, drop dist

ch.8 <- glm(binomial ~ Temp + Habitat, data = chum, family = binomial)
summary(ch.8)

anova(ch.7, ch.8, test = "Chisq") # keep salinity

ch.9 <- glm(binomial ~ Habitat + Salinity, data = chum, family = binomial)
summary(ch.9)

ch.9.up <- glm(binomial ~ Habitat + Salinity, data = chum[-28,], family = binomial)


anova(ch.7, ch.9.up, test = "Chisq") ## null accepted, drop Temp

ch.10 <- glm(binomial ~ Habitat, data = chum, family = binomial)
summary(ch.10)

ch.10.up <- glm(binomial ~ Habitat, data = chum[-c(5:7, 28),], family = binomial)

anova(ch.9, ch.10.up, test = "Chisq") # null accepted?

# when temperature is included in the model, wnat to include salinity, but when its dropped salinity should be dropped?

AICc(ch.10); AICc(ch.7);AICc(ch.9);AICc(ch.8)
 # best model to go on is just with habitat? but the one that included salinity is also close. 
# model ch.7, ch.9
```

Compare nonnested models
```{r}
AICc(ch.3);AICc(ch.7);AICc(ch.9)
# best models are ch.7 and ch.9 -- lets look at fit
par(mfrow = c(2,3))
plot(ch.9, which = 1:6)
```

Back transform models
```{r}
par(mfrow=c(1,2))
visreg(ch.9, scale = "response")
```

## Negative binomial
Still don't want to include temperature and julian day in the same model. 

1. Julian day + julian 2 + salinity + dist + Habitat
```{r}

ch.nb <- glm.nb(log_abund ~ julian + I(julian^2) + Salinity + dist_km + Habitat,
             data = chum)
summary(ch.nb)

ch.nb2 <- glm.nb(log_abund ~ julian + Salinity + dist_km + Habitat,
             data = chum)
summary(ch.nb2)

lrtest(ch.nb, ch.nb2) # null not rejected, drop julain2

ch.nb3 <- glm.nb(log_abund ~ julian + Salinity + Habitat, data = chum)
summary(ch.nb3)

lrtest(ch.nb2, ch.nb3) # null not rejected, drop dist

ch.nb4 <- glm.nb(log_abund ~ julian + Habitat, data = chum)
summary(ch.nb4)
ch.nb4.up <- glm.nb(log_abund ~ julian + Habitat, data = chum[-c(5:7),])

lrtest(ch.nb3, ch.nb4.up) # drop salinity

ch.nb5 <- glm.nb(log_abund ~ Habitat, data = chum)
summary(ch.nb5)

lrtest(ch.nb4, ch.nb5) # drop julian hmmmm sorta need to keep it

AICc(ch.nb4); AICc(ch.nb5); AICc(ch.nb3);AICc(ch.nb2); AICc(ch.nb)
# looks liek for this set of nested models ch.nb5 and ch.nb3 are best
```

2. Temp + salinity + dist + Habitat
```{r}
ch.nb6 <- glm.nb(log_abund ~ Temp + Salinity + dist_km + Habitat, data = chum)
summary(ch.nb6)

ch.nb7 <- glm.nb(log_abund ~ Temp + Salinity + Habitat, data = chum)
summary(ch.nb7)

lrtest(ch.nb6, ch.nb7) # drop dist

ch.nb8 <- glm.nb(log_abund ~ Temp + Habitat, data = chum)
summary(ch.nb8)

lrtest(ch.nb7, ch.nb8) # almost sign consider keeping salinity?

ch.nb9 <- glm.nb(log_abund ~ Salinity + Habitat, data = chum)
summary(ch.nb9)

ch.nb9.up <- glm.nb(log_abund ~ Salinity + Habitat, data = chum[-28,])

lrtest(ch.nb7, ch.nb9.up) # drop temp

ch.nb10 <- glm.nb(log_abund ~ Habitat, data = chum)
summary(ch.nb10)
ch.nb10.up <- glm.nb(log_abund ~ Habitat, data = chum[-c(5:7),])

lrtest(ch.nb9, ch.nb10.up) # drop salinity? 

# Not really a clear what parameters to keep lets look at AIC
AICc(ch.nb7); AICc(ch.nb8); AICc(ch.nb9); AICc(ch.nb10);AICc(ch.nb6)

# looks like ch.nb8 and ch.nb9 are the best models here
```

Compare
```{r}
AICc(ch.nb8); AICc(ch.nb9);AICc(ch.nb5); AICc(ch.nb3)
# best are ch.nb8 and ch.nb9, lets look at diagnostics

par(mfrow=c(2,3))
plot(ch.nb8, which = 1:6)

par(mfrow=c(2,3))
plot(ch.nb9, which = 1:6)

summary(ch.nb8)
summary(ch.nb9)
```

More diagnositc plots
```{r}
par(mfrow = c(2,2))
d1 <- plot(chum[-c(5:7, 28),]$Temp, resid(ch.nb8))
abline(h =c(0, 1, -1), lty =c(1, 2, 2))
d5 <- plot(chum[-c(5:7, 28),]$Habitat, resid(ch.nb8))


d4 <- plot(chum[-c(5:7),]$Salinity, resid(ch.nb9))
abline(h =c(0, 1, -1), lty =c(1, 2, 2))
d6 <- plot(chum[-c(5:7),]$Habitat, resid(ch.nb9))

```

Comparing log/w/olog
```{r}
ch.nb11 <- glm.nb(abundance ~ Temp + Habitat, data = chum)
ch.nb12 <- glm.nb(abundance ~ Salinity + Habitat, data = chum)

par(mfrow = c(2,2))
d1 <- plot(chum[-c(5:7, 28),]$Temp, resid(ch.nb8))
d5 <- plot(chum[-c(5:7, 28),]$Habitat, resid(ch.nb8))

d2 <- plot(chum[-c(5:7, 28),]$Temp, resid(ch.nb11))
d6 <- plot(chum[-c(5:7, 28),]$Habitat, resid(ch.nb11))

par(mfrow = c(2,2))
d3 <- plot(chum[-c(5:7),]$Salinity, resid(ch.nb9))
d5 <- plot(chum[-c(5:7),]$Habitat, resid(ch.nb9))

d4 <- plot(chum[-c(5:7),]$Salinity, resid(ch.nb12))
d6 <- plot(chum[-c(5:7),]$Habitat, resid(ch.nb12))

# from visual inspection the models using log_abund look better in the residauls and fit. 
```

### back transforming ch.nb8 and ch.nb9
ch.nb8 first
```{r}
chum1 <- visreg(ch.nb8, "Temp", scale = "response", gg = TRUE, partial = TRUE, 
                alpha = .05, line = list(col = "black")) + 
  plot_theme() +
  xlab("Temperature (C)") + ylab("Chum salmon CPUE") + 
  geom_point() + coord_cartesian(ylim = c(0, 10))
chum1

chum2 <- visreg(ch.nb8, "Habitat", scale = "response", gg = TRUE, 
                partial = TRUE, alpha = 0.5, line = list(col = "black")) + 
  plot_theme() + coord_cartesian(ylim = c(0, 10)) +
  xlab("Habitat") + ylab("Chum salmon CPUE")
chum2

```

ch.nb9 second
```{r}
chum3 <- visreg(ch.nb9, "Salinity", scale = "response", gg = TRUE, partial = TRUE, 
                alpha = .05, line = list(col = "black")) + 
  plot_theme() +
  xlab("Salinity") + ylab("Chum salmon CPUE") + 
  geom_point() + coord_cartesian(ylim = c(0, 10))
chum3

chum4 <- visreg(ch.nb9, "Habitat", scale = "response", gg = TRUE, 
                partial = TRUE, alpha = 0.5, line = list(col = "black")) + 
  plot_theme() + coord_cartesian(ylim = c(0, 10)) +
  xlab("Habitat") + ylab("Chum salmon CPUE")
chum4

visreg(ch.nb3, "julian", scale = "response")
```

plot together
```{r}
plot_grid(chum1, chum2, chum3, chum4,
          labels= c("ch.nb8", "", "ch.nb9", ""),
          nrow = 2, ncol = 2)
```

